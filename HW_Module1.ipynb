{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MCP Homework and Local LLM Implementation with Ollama & LangChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Using MCP to built Agent-like work-flow.\n",
    "\n",
    "**Task 1.1 MCP + Claude = Browser Automation**\\\n",
    "\n",
    "\n",
    "1. Use Brave Search to:\n",
    "Task: Search for “latest AI paper publication platforms” and list the top 3 search results with titles and URLs.\n",
    "Prompt in Claude:\n",
    "\"Use Brave Search to look up the latest AI paper publication platforms and return the top 3 results with title and link.\"\n",
    "\n",
    "*note: no API key was used as brave search requires credit card for free plan*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](bravesearch-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use GitHub to:\n",
    "Task: Access one of your public repositories (e.g., my-cool-project) and list the 5 most recent commits.\n",
    "Prompt in Claude:\n",
    "\"Connect to my GitHub account using the MCP plugin and list the 5 latest commits from the repository my-cool-project.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](github-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use Puppeteer to:\n",
    "Task: Visit https://www.inference.ai/, take a full-page screenshot, and save it as example.png.\n",
    "Prompt in Claude:\n",
    "\"Use Puppeteer to go to https://www.inference.ai/ and capture a full-page screenshot saved as example.png.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](puppeteer-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use Filesystem to:\n",
    "Task: Create a new folder on your Desktop named mcp_test, and inside it, create a text file hello.txt containing “Hello MCP!”.\n",
    "Prompt in Claude:\n",
    "\"Use Filesystem to create a folder named mcp_test on my Desktop and add a file hello.txt inside with the text 'Hello MCP!'.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](filesystem-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Use Sequential Thinking to:\n",
    "Task: Think step-by-step about how to prepare for a technical interview and generate a preparation plan.\n",
    "Prompt in Claude:\n",
    "\"Use Sequential Thinking to create a step-by-step plan for preparing for a technical interview.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](sequentialthinking-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Use Notion to:\n",
    "Task: Create a new Notion page titled “MCP Automation Test” and log the results of all the tasks above.\n",
    "Prompt in Claude:\n",
    "\"Use the Notion plugin to create a new page titled 'MCP Automation Test' and write a summary of the tasks I just completed using each plugin.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](notion-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Task: Use Claude + Puppeteer to automatically visit a webpage, scrape table content, and save it locally (with the help of the Filesystem plugin).\n",
    "Project Management Workflow: Record the scraped and analyzed data into a Notion database, automatically generating documentation.\n",
    "\n",
    "![alt text](advanced_task-1.png)\n",
    "\n",
    "\n",
    "https://www.notion.so/Wikipedia-Baseball-Information-Scraper-29913692a10d810fa882e6fd40c949e3\n",
    "This workflow demonstrates a fully automated pipeline from data extraction to documentation, making it easy to manage and track multiple web scraping projects! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Play with Ollama\n",
    "\n",
    "<img src=\"https://ollama.com/public/blog/meta-ollama-llama3.png\" alt=\"jupyter\" width=\"500\"/>\n",
    "\n",
    "***Ollama** is a **convenient** and **free** framework，designed for easy deployment and running of large language models (LLMs) locally.*  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Task 2.1: Install Ollama and run LLMs locally**  \n",
    "- refer to [Ollama](https://ollama.ai/) to complete installation.  \n",
    "- Run `ollama run llama2` from the command line to download and launch the `llama2` model.\n",
    "\n",
    "\n",
    "**Task 2.2: Using Ollama to call OpenAI API**\\\n",
    "*Ollama now has built-in compatibility with the OpenAI Chat Completions API, making it possible to use more tooling and applications with Ollama locally.*\n",
    "\n",
    "See official instruction below：\\\n",
    "[Ollama OpenAI Compatibility](https://ollama.com/blog/openai-compatibility)\\\n",
    "[Ollama OpenAI](https://github.com/ollama/ollama/blob/main/docs/openai.md)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To invoke Ollama’s OpenAI compatible API endpoint, use the same OpenAI format and change the hostname to http://localhost:11434:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curl Method:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "curl http://localhost:11434/v1/chat/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"llama2\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hello!\"\n",
    "            }\n",
    "        ]\n",
    "    }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"llama2\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The LA Dodgers won in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In above examples, “Ollama” is essentially acting as a local server that is compatible with the OpenAI API. In other words, the endpoint you’re calling—whether via code or a cURL command—is not the official OpenAI endpoint at https://api.openai.com/v1/ but rather http://localhost:11434/v1/. The local process running on this port is Ollama.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 Combining LangChain with Ollama’s local LLMs\n",
    "\n",
    "*LangChain simplifies every stage of the LLM application lifecycle.\n",
    "It unifies functional modules such as \"Prompt design\", \"Multi-round dialogue memory (Memory)\", \"External data retrieval (Retrieval)\" and \"Tools/Agents\" into a unified package. In this way, you do not need to manually manage each step of the language model call and data flow, and only need to focus on business logic.*\n",
    "\n",
    "LangChain–Ollama Documentation: [https://python.langchain.com/docs/integrations/llms/ollama](https://python.langchain.com/docs/integrations/llms/ollama)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*-PlFCd_VBcALKReO3ZaOEg.png\" alt=\"jupyter\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.1 Reproduce practice in lecture using LCEL**\n",
    "\n",
    "*A chain is a sequence of steps or model calls connected together to achieve a larger task. Each step can involve retrieving information, transforming text, or invoking a language model in some way, and then passing its output on to the next step in the chain. This structure helps you build more complex workflows or pipelines using multiple actions in a simple, organized manner.*\n",
    "\n",
    "- what is LCEL? LCEL is a much simpler way to construct \"Chain\"\\\n",
    "[LCEL](https://python.langchain.com/docs/concepts/lcel/)\\\n",
    "why use it?\\\n",
    "[Why LCEL](https://python.langchain.com/v0.1/docs/expression_language/why/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code with a Ollama local model："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using LCEL to reproduce a \"Basic Prompting\" scenario\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama \n",
    "\n",
    "# 2. Define the prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"What is the capital of {topic}?\"\n",
    ")\n",
    "\n",
    "# 3. Define the model\n",
    "model = ChatOllama(model = [\"llama2\"])  # Using Ollama \n",
    "\n",
    "# 4. Chain the components together using LCEL\n",
    "chain = (\n",
    "    # LCEL syntax: use the pipe operator | to connect each step\n",
    "    {\"topic\": RunnablePassthrough()}  # Accept user input\n",
    "    | prompt                          # Transform it into a prompt message\n",
    "    | model                           # Call the model\n",
    "    | StrOutputParser()               # Parse the output as a string\n",
    ")\n",
    "\n",
    "# 5. Execute\n",
    "result = chain.invoke(\"Germany\")\n",
    "print(\"User prompt: 'What is the capital of Germany?'\")\n",
    "print(\"Model answer:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Submission Requirements**  \n",
    "1. Complete all tasks.  \n",
    "2. Include screenshots of key outputs (e.g., model responses, agent computation results).\n",
    "\n",
    "- Advance Work：Integrate the Ollama and Langchain tasks into **Gradio Web UI**, which will be useful for building Proxy AI-Agent interface translation with front-end, and demonstrate your work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1/\",\n",
    "    api_key=\"\"\n",
    ")\n",
    "\n",
    "def chat(message, info):\n",
    "    client.chat.completions.create(messages=info, model=\"llama2\")\n",
    "    return []\n",
    "\n",
    "def info():\n",
    "    return [\n",
    "        {\"role\": \"assistant\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The LA Dodgers won in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    "\n",
    "demo = gr.ChatInterface(chat, type=\"messages\")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
