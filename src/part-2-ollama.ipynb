{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with Ollama Locally Using Python\n",
    "\n",
    "This notebook demonstrates how to set up and interact with a local Ollama instance using Python. It covers two primary methods: using the OpenAI-compatible API endpoint and using the native `ollama` Python package.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [1. Setup](#1.-Setup)\n",
    "  - [1.1. Verify Ollama Installation](#1.1.-Verify-Ollama-Installation)\n",
    "  - [1.2. Install Python Dependencies](#1.2.-Install-Python-Dependencies)\n",
    "- [2. Interacting with Ollama](#2.-Interacting-with-Ollama)\n",
    "  - [2.1. Using the OpenAI-Compatible Endpoint](#2.1.-Using-the-OpenAI-Compatible-Endpoint)\n",
    "  - [2.2. Using the `ollama` Python Package](#2.2.-Using-the-ollama-Python-Package)\n",
    "    - [Basic Chat](#Basic-Chat)\n",
    "    - [Streaming Responses](#Streaming-Responses)\n",
    "    - [Handling Streamed Chunks](#Handling-Streamed-Chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Verify Ollama Installation\n",
    "\n",
    "First, ensure that Ollama is installed and running on your local machine. You can verify this by listing the available models from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Install Python Dependencies\n",
    "\n",
    "To interact with Ollama, you'll need to install the necessary Python libraries. This notebook uses Conda for environment management.\n",
    "\n",
    "#### OpenAI Library\n",
    "The `openai` library allows you to connect to Ollama's OpenAI-compatible endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda install --name hw-1-submission openai -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ollama Library\n",
    "For a more direct approach, you can use the native `ollama` Python package. Since it's available on `conda-forge`, you may need to add the channel first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda config --add channels conda-forge\n",
    "! conda config --set channel_priority strict\n",
    "! conda install --name hw-1-submission ollama -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Interacting with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Using the OpenAI-Compatible Endpoint\n",
    "\n",
    "Ollama provides a local server that mimics the OpenAI API. This allows you to use the `openai` Python client to interact with your local models by changing the `base_url`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama',  # required, but unused\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"smollm:1.7b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The LA Dodgers won in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Using the `ollama` Python Package\n",
    "\n",
    "The official `ollama` package provides a more direct way to interact with the Ollama server. Below are a few examples of its usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Chat\n",
    "\n",
    "This is the simplest way to send a prompt and receive a complete response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat, ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='smollm:1.7b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Why is the sky blue?',\n",
    "    },\n",
    "])\n",
    "\n",
    "# You can access the response content as a dictionary key\n",
    "print(response['message']['content'])\n",
    "\n",
    "# Or as a field from the response object\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming Responses\n",
    "\n",
    "For long-running requests, you can stream the response as it's being generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "stream = chat(\n",
    "    model='smollm:1.7b',\n",
    "    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Streamed Chunks\n",
    "\n",
    "This example shows a more advanced way to handle streamed responses, including capturing the model's \"thinking\" process if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "stream = chat(\n",
    "    model='smollm:1.7b',\n",
    "    messages=[{'role': 'user', 'content': 'What is 17 Ã— 23?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "in_thinking = False\n",
    "content = ''\n",
    "thinking = ''\n",
    "\n",
    "for chunk in stream:\n",
    "    if hasattr(chunk['message'], 'thinking') and chunk['message']['thinking']:\n",
    "        if not in_thinking:\n",
    "            in_thinking = True\n",
    "            print('Thinking:\\n', end='', flush=True)\n",
    "        print(chunk['message']['thinking'], end='', flush=True)\n",
    "        thinking += chunk['message']['thinking']\n",
    "    elif chunk['message']['content']:\n",
    "        if in_thinking:\n",
    "            in_thinking = False\n",
    "            print('\\n\\nAnswer:\\n', end='', flush=True)\n",
    "        print(chunk['message']['content'], end='', flush=True)\n",
    "        content += chunk['message']['content']\n",
    "\n",
    "# The final response can be appended to the message history for context in future requests\n",
    "new_messages = [{'role': 'assistant', 'thinking': thinking, 'content': content}]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
