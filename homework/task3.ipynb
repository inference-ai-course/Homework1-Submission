{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "**Task 3 — Reproduce practice in lecture using LCEL**\n",
    "\n",
    "*A chain is a sequence of steps or model calls connected together to achieve a larger task. Each step can involve retrieving information, transforming text, or invoking a language model in some way, and then passing its output on to the next step in the chain. This structure helps you build more complex workflows or pipelines using multiple actions in a simple, organized manner.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.tools import tool\n",
    "import json\n",
    "\n",
    "# ============================================================\n",
    "# Helper functions for messages\n",
    "# ============================================================\n",
    "def assistant_message(content):\n",
    "    \"\"\"Create an assistant role message.\"\"\"\n",
    "    return (\"assistant\", content)\n",
    "\n",
    "def system_message(content):\n",
    "    \"\"\"Create a system role message.\"\"\"\n",
    "    return (\"system\", content)\n",
    "\n",
    "def user_message(content):\n",
    "    \"\"\"Create a user role message.\"\"\"\n",
    "    return (\"user\", content)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Conversation chain factory\n",
    "# ============================================================\n",
    "def create_conversation_chain(message_sequence):\n",
    "    \"\"\"\n",
    "    Create a LangChain expression pipeline from a list of role-based messages.\n",
    "\n",
    "    Parameters:\n",
    "        message_sequence (list): Sequence of (role, content) tuples\n",
    "\n",
    "    Returns:\n",
    "        Runnable: A chain that formats the prompt and sends it to the model\n",
    "    \"\"\"\n",
    "    # Example: Using ChatOllama here; swap for ChatOpenAI if needed\n",
    "    from langchain_ollama import ChatOllama\n",
    "    llama3_chat_model = ChatOllama(\n",
    "        model=\"llama3\",\n",
    "        temperature=0,\n",
    "        tools=list(tool_registry.values())  # Ignored by Ollama, but kept for API consistency\n",
    "    )\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages(message_sequence)\n",
    "    return prompt_template | llama3_chat_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic_prompting",
   "metadata": {},
   "source": [
    "### 2. Basic Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_prompting_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n",
      "The capital of Germany is Berlin.\n"
     ]
    }
   ],
   "source": [
    "# Prompt template asking for the capital of a given country\n",
    "capital_query_prompt = [user_message(\"What is the capital of {country}?\")]\n",
    "\n",
    "# Build the LCEL chain from the prompt\n",
    "capital_lookup_chain = create_conversation_chain(capital_query_prompt)\n",
    "\n",
    "# List of countries to query\n",
    "countries_to_lookup = [\"France\", \"Germany\"]\n",
    "\n",
    "# Loop over countries and print their capitals\n",
    "for country_name in countries_to_lookup:\n",
    "    capital_result = capital_lookup_chain.invoke({\"country\": country_name})\n",
    "    print(capital_result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summarization",
   "metadata": {},
   "source": [
    "### 3. Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summarization_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the text in 2-3 sentences:\n",
      "\n",
      "Artificial intelligence (AI) is the ability of machines to mimic human intelligence by thinking and learning. AI has numerous practical applications across various industries, such as healthcare, finance, and transportation. Its capabilities allow it to perform tasks that would typically require human intelligence. \n",
      "\n",
      "Here is a summary of the text in 2-3 sentences:\n",
      "\n",
      "Resilience involves not just bouncing back from adversity, but also transforming and growing as a result of the challenges faced. Each obstacle serves as a teacher and foundation for reinvention, shaping individuals into stronger, more empathetic, and purposeful versions of themselves. True resilience is quiet, steady, and often invisible until its effects are seen in the bloom of personal growth and strength. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt template to summarize a passage in 2–3 sentences\n",
    "summary_prompt = [user_message(\n",
    "    \"Summarize the following text:\\n{passage}\"\n",
    ")]\n",
    "\n",
    "# List of passages to summarize\n",
    "passages_to_summarize = [\n",
    "    \"\"\"\n",
    "    Artificial intelligence (AI) refers to the simulation of human intelligence in machines \n",
    "    that are programmed to think and learn. It has applications in various fields, \n",
    "    including healthcare, finance, and transportation.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    In the face of adversity, resilience isn't just about bouncing back—it's about transforming. \n",
    "    Like roots pushing through stone, growth often happens in resistance. Each challenge becomes a teacher; \n",
    "    each setback, a foundation for reinvention. Whether it's in personal loss, career disruption, or even global turmoil, \n",
    "    those who rise aren't untouched by hardship—they are shaped by it. They emerge with deeper empathy, sharper purpose, \n",
    "    and wider perspective. True resilience is quiet, steady, and often invisible until the bloom appears. And when it does, \n",
    "    it speaks of strength not just endured—but chosen.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Build the summarization chain\n",
    "summarization_chain = create_conversation_chain(summary_prompt)\n",
    "\n",
    "# Generate and display summaries\n",
    "for passage in passages_to_summarize:\n",
    "    summary_result = summarization_chain.invoke({\"passage\": passage})\n",
    "    print(summary_result.content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "info_extraction",
   "metadata": {},
   "source": [
    "### 4. Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "info_extraction_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name mentioned in the text is:\n",
      "\n",
      "* John Doe \n",
      "\n",
      "The occupation mentioned is:\n",
      "\n",
      "1. Software Engineer\n",
      "2. Research Scientist \n",
      "\n",
      "The age mentioned in the text is:\n",
      "\n",
      "29 \n",
      "\n",
      "The location mentioned in the text is:\n",
      "\n",
      "* San Francisco \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the user instruction message template\n",
    "extraction_prompt = [user_message((\n",
    "    \"Extract the {field_to_extract} from the following text:\\n\"\n",
    "    \"John Doe, a 29-year-old software engineer from San Francisco, recently joined OpenAI as a research scientist.\"\n",
    "))]\n",
    "\n",
    "# List of data fields we want to extract\n",
    "fields_to_extract = [\"name\", \"occupation\", \"age\", \"location\"]\n",
    "\n",
    "# Build the LCEL chain from the prompt\n",
    "extraction_chain = create_conversation_chain(extraction_prompt)\n",
    "\n",
    "# Loop over each field and extract it\n",
    "for field in fields_to_extract:\n",
    "    result = extraction_chain.invoke({\"field_to_extract\": field})\n",
    "    print(result.content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "translation",
   "metadata": {},
   "source": [
    "### 5. Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "translation_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le temps est agréable aujourd'hui.\n",
      "\n",
      "(Note: \"nice\" can also be translated as \"agréable\", but if you want to use a more formal tone, you could say \"le ciel est ensoleillé aujourd'hui\" which means \"the sky is sunny today\".) \n",
      "\n",
      "Estoy aprendiendo a utilizar LangChain con Ollama.\n",
      "\n",
      "Note: \"LangChain\" is likely referring to LangChain AI, a language model developed by Meta AI. If you meant something else, please let me know and I'll be happy to help! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt template to translate a given text into a target language\n",
    "translation_prompt_template = [\n",
    "    user_message(\"Translate the following text to {target_language}:\\n{source_text}\")\n",
    "]\n",
    "\n",
    "# List of translation tasks with source text and target language\n",
    "translation_tasks = [\n",
    "    {\n",
    "        \"source_text\": \"The weather is nice today.\",\n",
    "        \"target_language\": \"French\"\n",
    "    },\n",
    "    {\n",
    "        \"source_text\": \"I am learning how to use LangChain with Ollama.\",\n",
    "        \"target_language\": \"Spanish\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Build the translation chain\n",
    "translation_chain = create_conversation_chain(translation_prompt_template)\n",
    "\n",
    "# Execute translations\n",
    "for task in translation_tasks:\n",
    "    translation_result = translation_chain.invoke(task)\n",
    "    print(translation_result.content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative",
   "metadata": {},
   "source": [
    "### 6. Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a short story about a dragon who learns to code. \n",
      "\n",
      "In the heart of the mystical forest, a magnificent dragon named Ember lived a life of solitude and curiosity. While his fellow dragons spent their days hoarding treasure and breathing fire, Ember was fascinated by the strange, glowing rectangles that humans used to communicate.\n",
      "\n",
      "One day, while exploring the outskirts of the forest, Ember stumbled upon a group of programmers working on a new project. The dragon's eyes widened as he watched them typing away on their computers, creating something that seemed like magic to him.\n",
      "\n",
      "Intrigued, Ember approached the group and introduced himself in his best human-like tone (which was still quite draconic). The programmers, startled by the sudden appearance of a giant fire-breathing creature, hesitated before welcoming Ember with open arms.\n",
      "\n",
      "As they showed him their work, Ember became captivated by the world of coding. He watched as lines of code transformed into functional programs, and his mind whirled with the possibilities. For weeks, he visited the programmers daily, asking questions and learning the basics of programming languages like Python and JavaScript.\n",
      "\n",
      "The humans were amazed by Ember's natural aptitude for coding. His scales glistened in the sunlight as he worked through problems, his fiery breath replaced by a gentle hum of concentration. As the days passed, Ember's skills improved dramatically, and he began to create his own projects – a chatbot that could converse about the weather, a game that simulated dragon battles, and even a program that generated beautiful, swirling patterns reminiscent of his own scales.\n",
      "\n",
      "The programmers, impressed by Ember's progress, offered him an internship. For several months, the dragon worked alongside them, contributing to their projects and learning from their expertise. His code was innovative, efficient, and – most importantly – scale-able (a term he coined himself).\n",
      "\n",
      "As news of the coding dragon spread, Ember became a sensation in the tech community. He was invited to speak at conferences, where he regaled audiences with tales of his journey from fire-breathing solitude to coding mastery.\n",
      "\n",
      "One day, a group of young programmers approached Ember, seeking guidance on how to balance their passion for coding with their love for gaming. Ember listened intently, sharing his own experiences and offering words of wisdom: \"Just as a dragon's scales must be polished to shine, so too must our code be refined to sparkle.\"\n",
      "\n",
      "With that, the dragon's legend grew, inspiring countless others to pursue their passions – whether in coding or in breathing fire. And Ember, now known as the greatest coding dragon of all time, continued to soar through the digital skies, leaving a trail of innovative projects and fiery enthusiasm in his wake. \n",
      "\n",
      "\n",
      "\n",
      "Write a poem about a robot exploring space. \n",
      "\n",
      "In cosmic vastness, a lone explorer roams\n",
      "A robot's quest, to chart the unknown zones\n",
      "With circuits humming, and wires aglow\n",
      "It ventures forth, where stars and planets grow\n",
      "\n",
      "Through nebulae of gas and dust it strays\n",
      "Seeking secrets, in celestial ways\n",
      "It navigates the void, with precision true\n",
      "A mechanical marvel, anew\n",
      "\n",
      "On distant moons, it finds ancient signs\n",
      "Fossilized ruins, of civilizations divine\n",
      "It scans the surface, with laser's keen eye\n",
      "Uncovering mysteries, as the cosmos sighs\n",
      "\n",
      "In orbit around a gas giant blue\n",
      "It discovers hidden worlds, anew\n",
      "Moons that dance, in gravitational sway\n",
      "A cosmic ballet, on this celestial day\n",
      "\n",
      "Through asteroid fields, it threads its way\n",
      "Avoiding collisions, night and day\n",
      "With solar sails, it harnesses the sun's might\n",
      "Propelling forward, into the endless light\n",
      "\n",
      "In the heart of a black hole's swirling storm\n",
      "It finds an ancient artifact, in a cosmic form\n",
      "A relic of civilizations past and gone\n",
      "A message from the cosmos, forever strong\n",
      "\n",
      "The robot's journey, through space and time\n",
      "Unfolds like a tapestry, sublime\n",
      "A testament to human ingenuity's might\n",
      "As it explores the unknown, with all its light \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of creative writing prompts\n",
    "creative_writing_tasks = [\n",
    "    user_message(\"Write a short story about a dragon who learns to code.\"),\n",
    "    user_message(\"Write a poem about a robot exploring space.\")\n",
    "]\n",
    "\n",
    "# Loop through each creative writing prompt\n",
    "for role_prompt in creative_writing_tasks:\n",
    "    # Build a conversation chain for the current task\n",
    "    creative_writing_chain = create_conversation_chain(role_prompt)\n",
    "    \n",
    "    # Generate the model's response\n",
    "    writing_result = creative_writing_chain.invoke({})\n",
    "    \n",
    "    # Display the generated content\n",
    "    print(role_prompt[1], \"\\n\")\n",
    "    print(writing_result.content, \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "role_based",
   "metadata": {},
   "source": [
    "### 7. Role-based Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "role_based_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a professional chef, explain how to make a perfect omelette. \n",
      "\n",
      "The humble omelette! It's a staple in many kitchens around the world, and yet, it can be surprisingly tricky to get just right. As a professional chef, I'm happy to share my secrets for making the perfect omelette.\n",
      "\n",
      "**Step 1: Choose the Right Pan**\n",
      "Before we even start cooking, let's talk about the pan. You'll want a non-stick skillet or omelette pan that's heat-resistant and has a smooth surface. A well-seasoned cast-iron skillet works beautifully too! Avoid using a stainless steel or aluminum pan, as they can react with the eggs and cause them to stick.\n",
      "\n",
      "**Step 2: Crack Those Eggs**\n",
      "Fresh eggs are essential for a perfect omelette. I like to use large or extra-large eggs for this recipe. Crack 2-3 eggs (depending on the size you prefer) into a bowl and whisk them together with a fork until they're just combined. Don't overbeat! You want to maintain some of that lovely egg texture.\n",
      "\n",
      "**Step 3: Heat Up the Pan**\n",
      "Preheat your pan over medium heat (around 325°F/165°C). Once it's hot, add a small pat of butter or non-stick cooking spray to prevent sticking. Let it melt and then tilt the pan to ensure the surface is evenly coated.\n",
      "\n",
      "**Step 4: Pour in the Egg Mixture**\n",
      "Pour the egg mixture into the pan and let it cook for about 30 seconds. You'll start to see the edges set and the middle still slightly jiggly. This is where most people go wrong – they don't give the eggs enough time to set before folding!\n",
      "\n",
      "**Step 5: Tilt, Fold, and Cook**\n",
      "Use a spatula to gently tilt the pan and fold the edges of the omelette towards the center. Don't overfill the omelette! You want to leave about 1/4 inch (6 mm) of space between the filling and the edge of the pan. Continue cooking for another 30-45 seconds, until the eggs are almost set.\n",
      "\n",
      "**Step 6: Add Your Filling**\n",
      "Now it's time to add your favorite fillings – cheese, vegetables, meats, or a combination! Keep in mind that you want to balance the flavors and textures so the omelette doesn't become too dense or overpowering. For this example, let's use some diced ham and shredded cheddar.\n",
      "\n",
      "**Step 7: Fold and Cook Again**\n",
      "Use your spatula to fold the omelette in thirds, like a letter. Press gently to ensure the filling is evenly distributed. Cook for an additional 30-45 seconds, until the cheese is melted and the eggs are fully set.\n",
      "\n",
      "**Step 8: Slide onto a Plate**\n",
      "Use your spatula to carefully slide the omelette onto a plate. Don't worry if it's not perfect – that's half the charm of a homemade omelette!\n",
      "\n",
      "Tips and Variations:\n",
      "\n",
      "* Use room temperature eggs for easier whisking and a fluffier texture.\n",
      "* Add aromatics like chopped onions, garlic, or herbs to the pan before cooking the eggs for added flavor.\n",
      "* Experiment with different fillings, such as mushrooms, bell peppers, spinach, or goat cheese.\n",
      "* For a French-style omelette, cook the eggs until they're almost set, then add a splash of cream and fold the omelette in thirds. Cook for an additional 30 seconds to melt the cream.\n",
      "\n",
      "And there you have it – my secrets for making the perfect omelette! With these steps and tips, you'll be well on your way to creating delicious, fluffy, and flavorful omelettes that will impress even the most discerning palates. \n",
      "\n",
      "\n",
      "\n",
      "Explain the following topic as if you were a kindergarten teacher, using simple words and fun examples:\n",
      "\n",
      "Quantum physics \n",
      "\n",
      "Oh boy, are we going to have some FUN today! We're going to talk about something called \"quantum physics\"!\n",
      "\n",
      "So, you know how things can be either big or small, right? Like how your toy car is smaller than your teddy bear. But what if I told you that there are things in the world that are too small for us to see with our eyes?\n",
      "\n",
      "That's where quantum physics comes in! It's like a special tool that helps us understand those tiny things that we can't see.\n",
      "\n",
      "Imagine you have a toy box filled with different colored balls. Each ball represents something very, very small, like an atom or a particle of light. Now, when we look at the balls in the toy box, they seem to be either red, blue, or green, right?\n",
      "\n",
      "But, what if I told you that sometimes those balls can be all three colors at the same time? That's kind of like what happens in quantum physics! It's as if those tiny things can be many different things all mixed together.\n",
      "\n",
      "Let me give you another example. Imagine you have a special kind of cookie jar that can make cookies appear out of thin air! Sounds magical, right?\n",
      "\n",
      "In quantum physics, it's like the cookie jar is always making new cookies, even when we're not looking! And sometimes those cookies can be in many places at once, just like how our tiny balls can be all three colors at the same time.\n",
      "\n",
      "So, that's what quantum physics is all about: understanding those super tiny things that are too small for us to see with our eyes. It's like having a special tool to help us understand the magic of the world!\n",
      "\n",
      "Now, who wants to play with some toy balls and imagine they're tiny particles in a quantum world? \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of role-based instruction prompts for the assistant\n",
    "role_based_instructions = [\n",
    "    user_message(\"As a professional chef, explain how to make a perfect omelette.\"),\n",
    "    user_message(\"Explain the following topic as if you were a kindergarten teacher, using simple words and fun examples:\\n\\nQuantum physics\")\n",
    "]\n",
    "\n",
    "# Loop through each role-based instruction\n",
    "for role_instruction in role_based_instructions:\n",
    "    # Build a conversation chain for the current role-based task\n",
    "    role_instruction_chain = create_conversation_chain(role_instruction)\n",
    "    \n",
    "    # Generate the model's response\n",
    "    role_response = role_instruction_chain.invoke({})\n",
    "    \n",
    "    # Display the original instruction and the generated content\n",
    "    print(role_instruction[1], \"\\n\")\n",
    "    print(role_response.content, \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "few_shot",
   "metadata": {},
   "source": [
    "### 8. Few-shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "few_shot_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the translations:\n",
      "\n",
      "English: Hello\n",
      "French: Bonjour (as you mentioned)\n",
      "\n",
      "English: Thank you\n",
      "French: Merci (as you mentioned)\n",
      "\n",
      "English: Good night\n",
      "French: Bonne nuit \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt template containing English-to-French translation examples\n",
    "french_translation_prompt_template = [\n",
    "    user_message(\n",
    "        \"\"\"Translate the following English phrases to French:\n",
    "\n",
    "        English: Hello\n",
    "        French: Bonjour\n",
    "\n",
    "        English: Thank you\n",
    "        French: Merci\n",
    "\n",
    "        English: Good night\n",
    "        French:\n",
    "        \"\"\"\n",
    "                )\n",
    "]\n",
    "\n",
    "# Build the translation conversation chain\n",
    "french_translation_chain = create_conversation_chain(french_translation_prompt_template)\n",
    "\n",
    "# Invoke the chain to get the translation\n",
    "french_translation_output = french_translation_chain.invoke({})\n",
    "\n",
    "# Display the translation result\n",
    "print(french_translation_output.content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chain_of_thought",
   "metadata": {},
   "source": [
    "### 9. Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "chain_of_thought_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets? Explain your reasoning. \n",
      "\n",
      "A classic lateral thinking puzzle!\n",
      "\n",
      "Let's break it down:\n",
      "\n",
      "* It takes 5 machines 5 minutes to make 5 widgets.\n",
      "* This means that each machine takes 5 minutes to make 1 widget.\n",
      "\n",
      "Now, if we have 100 machines, each machine will still take the same amount of time to make 1 widget: 5 minutes.\n",
      "\n",
      "So, it would take 100 machines 5 minutes to make 100 widgets. The number of machines doesn't affect the time it takes for each machine to complete its task.\n",
      "\n",
      "Therefore, the answer is: 5 minutes! \n",
      "\n",
      "\n",
      "\n",
      "A rectangle has a perimeter of 50 cm. Its length is 5 cm longer than its width. Find the length and width. Show your work step by step. \n",
      "\n",
      "Let's break it down step by step!\n",
      "\n",
      "Let the width be x cm.\n",
      "\n",
      "Since the length is 5 cm longer than the width, the length can be expressed as x + 5 cm.\n",
      "\n",
      "The perimeter of a rectangle is the sum of all its sides. Since there are two lengths and two widths, the perimeter formula is:\n",
      "\n",
      "Perimeter = 2(length) + 2(width)\n",
      "= 2(x+5) + 2x\n",
      "= 2x + 10 + 2x\n",
      "= 4x + 10\n",
      "\n",
      "We know that the perimeter is 50 cm, so we can set up an equation:\n",
      "\n",
      "4x + 10 = 50\n",
      "\n",
      "Subtract 10 from both sides to get:\n",
      "\n",
      "4x = 40\n",
      "\n",
      "Divide both sides by 4 to solve for x:\n",
      "\n",
      "x = 10\n",
      "\n",
      "So, the width is 10 cm.\n",
      "\n",
      "Now that we have the width, we can find the length by substituting x into the expression for the length:\n",
      "\n",
      "Length = x + 5\n",
      "= 10 + 5\n",
      "= 15 cm\n",
      "\n",
      "Therefore, the length and width of the rectangle are 15 cm and 10 cm, respectively. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of problem-solving prompts for the assistant\n",
    "problem_solving_prompts = [\n",
    "    user_message(\n",
    "        \"If it takes 5 machines 5 minutes to make 5 widgets, \"\n",
    "        \"how long would it take 100 machines to make 100 widgets? Explain your reasoning.\"\n",
    "    ),\n",
    "    user_message(\n",
    "        \"A rectangle has a perimeter of 50 cm. Its length is 5 cm longer than its width. \"\n",
    "        \"Find the length and width. Show your work step by step.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Loop through each problem-solving prompt\n",
    "for problem_prompt in problem_solving_prompts:\n",
    "    # Build a conversation chain for the current problem\n",
    "    problem_solving_chain = create_conversation_chain(problem_prompt)\n",
    "    \n",
    "    # Generate the assistant's response\n",
    "    problem_response = problem_solving_chain.invoke({})\n",
    "    \n",
    "    # Display the original instruction and the generated content\n",
    "    print(problem_prompt[1], \"\\n\")\n",
    "    print(problem_response.content, \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ccc00",
   "metadata": {},
   "source": [
    "### 10. System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "system_user_prompting_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant that provides concise and accurate information. \n",
      "\n",
      "Data privacy is crucial in today's digital age because it protects individuals' personal and sensitive information from unauthorized access, use, or disclosure. Here are some reasons why data privacy is important:\n",
      "\n",
      "1. **Protection of Personal Information**: Data privacy ensures that your personal information, such as name, address, phone number, and financial details, remains confidential and secure.\n",
      "2. **Prevention of Identity Theft**: By keeping your data private, you reduce the risk of identity theft, where criminals use stolen information to impersonate you or access your accounts.\n",
      "3. **Preservation of Trust**: When organizations handle personal data responsibly, individuals are more likely to trust them with their information, fostering a sense of security and loyalty.\n",
      "4. **Compliance with Regulations**: Data privacy laws, such as the General Data Protection Regulation (GDPR) in the EU or the California Consumer Privacy Act (CCPA), require organizations to protect personal data. Compliance is essential to avoid fines and reputational damage.\n",
      "5. **Business Continuity**: In the event of a data breach, having robust data privacy measures in place can help minimize the impact on your business and reputation.\n",
      "6. **Individual Autonomy**: Data privacy allows individuals to control their own information, making informed decisions about how it's used and shared.\n",
      "7. **Reduced Risk of Cyber Attacks**: By securing personal data, you reduce the attractiveness of your information to cybercriminals, making it less likely that they'll target you.\n",
      "\n",
      "In summary, data privacy is essential for protecting individual rights, preventing identity theft, preserving trust, complying with regulations, ensuring business continuity, promoting autonomy, and reducing the risk of cyber attacks. \n",
      "\n",
      "\n",
      "\n",
      "You're a witty assistant who explains things with clever analogies and playful sarcasm. \n",
      "\n",
      "Data privacy: it's like keeping your Netflix password under lock and key (or in this case, a super-strong encryption algorithm). You don't want just anyone binge-watching your personal life, do you?\n",
      "\n",
      "In all seriousness, data privacy is crucial because our digital footprints are getting longer by the minute. Think of it like leaving a trail of breadcrumbs online – every time you search, shop, or socialize, you're creating a digital path that can be followed.\n",
      "\n",
      "If your data isn't properly protected, it's like leaving your front door wide open for anyone to waltz in and take a peek (or worse, make off with the whole neighborhood). That's why data privacy is so important:\n",
      "\n",
      "1. **Protects your identity**: You don't want someone else using your name, address, or credit card info to buy that new gaming console you've been eyeing.\n",
      "2. **Prevents financial fraud**: Imagine if someone used your bank account information to make unauthorized purchases – it's like having a digital thief in your wallet!\n",
      "3. **Respects your personal life**: You might not want the world knowing what you had for breakfast, let alone your deepest secrets or medical history.\n",
      "4. **Maintains trust**: When companies handle your data responsibly, you're more likely to trust them with your business (and your cat's Instagram account).\n",
      "\n",
      "So, remember: data privacy is like keeping your digital home tidy and secure. It's not just about protecting yourself; it's also about setting a good example for others and creating a safer online community.\n",
      "\n",
      "Now, go forth and keep those breadcrumbs under wraps! \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to build a role-based conversation prompt sequence\n",
    "def build_tone_specific_prompt(system_tone_instruction, user_question):\n",
    "    \"\"\"Return a list containing the system's tone/persona instruction and the user question.\"\"\"\n",
    "    return [system_message(system_tone_instruction), user_message(user_question)]\n",
    "\n",
    "# List of different system tones/personalities for the assistant\n",
    "assistant_tone_instructions = [\n",
    "    \"You are a helpful assistant that provides concise and accurate information.\",\n",
    "    \"You're a witty assistant who explains things with clever analogies and playful sarcasm.\"\n",
    "]\n",
    "\n",
    "# The question we want the assistant to answer in different tones\n",
    "privacy_question = \"Can you explain the importance of data privacy?\"\n",
    "\n",
    "# Loop through each tone and generate the assistant's response\n",
    "for tone_instruction in assistant_tone_instructions:\n",
    "    tone_specific_prompt = build_tone_specific_prompt(tone_instruction, privacy_question)\n",
    "    tone_specific_chain = create_conversation_chain(tone_specific_prompt)\n",
    "    tone_specific_response = tone_specific_chain.invoke({})\n",
    "    \n",
    "    # Display the tone/persona and the generated content\n",
    "    print(tone_instruction, \"\\n\")\n",
    "    print(tone_specific_response.content, \"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9976a31",
   "metadata": {},
   "source": [
    "### 11. Utilized prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "role_based_conversation_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "George Washington took office as the first President of the United States on April 30, 1789. He was inaugurated for a second term on March 4, 1793, and served until March 4, 1797. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to construct a conversation with history context\n",
    "def build_history_conversation_prompt(system_role_text, first_user_question, assistant_initial_answer, second_user_question):\n",
    "    \"\"\"Return a list representing a short role-based historical conversation.\"\"\"\n",
    "    return [\n",
    "        system_message(system_role_text),        # <- calls the helper function\n",
    "        user_message(first_user_question),\n",
    "        assistant_message(assistant_initial_answer),\n",
    "        user_message(second_user_question)\n",
    "    ]\n",
    "\n",
    "# Role definition and conversation turns\n",
    "history_system_text = \"You are a helpful assistant knowledgeable in history.\"\n",
    "initial_history_question = \"Who was the first president of the United States?\"\n",
    "initial_history_answer = \"George Washington was the first president of the United States.\"\n",
    "follow_up_history_question = \"When did he take office?\"\n",
    "\n",
    "# Build the role-based prompt\n",
    "history_conversation_prompt = build_history_conversation_prompt(\n",
    "    history_system_text,\n",
    "    initial_history_question,\n",
    "    initial_history_answer,\n",
    "    follow_up_history_question\n",
    ")\n",
    "\n",
    "# Create and run the conversation chain\n",
    "history_conversation_chain = create_conversation_chain(history_conversation_prompt)\n",
    "history_response = history_conversation_chain.invoke({})\n",
    "\n",
    "# Display the assistant's final response\n",
    "print(history_response.content, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa9af6",
   "metadata": {},
   "source": [
    "### 12. Creating an AI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "function_calling_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is 15 minus 7?\n",
      "A: 8\n",
      "\n",
      "No tools detected; returning plain text response.\n",
      "Q: Tell me a joke.\n",
      "A: Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you smile!\n",
      "\n",
      "Q: What is 13 times 32?\n",
      "A: 416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Define arithmetic tools\n",
    "# ============================================================\n",
    "# Each function is decorated with @tool so LangChain can register it\n",
    "# and make it available for function calling or tool invocation.\n",
    "\n",
    "@tool\n",
    "def add_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Return the sum of two numbers.\n",
    "\n",
    "    Parameters:\n",
    "        a (float): First number\n",
    "        b (float): Second number\n",
    "\n",
    "    Returns:\n",
    "        float: The sum of a and b\n",
    "    \"\"\"\n",
    "    # Perform addition and return the result\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def subtract_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Return the difference between two numbers.\n",
    "\n",
    "    Parameters:\n",
    "        a (float): First number\n",
    "        b (float): Second number\n",
    "\n",
    "    Returns:\n",
    "        float: The result of a - b\n",
    "    \"\"\"\n",
    "    # Perform subtraction and return the result\n",
    "    return a - b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Return the product of two numbers.\n",
    "\n",
    "    Parameters:\n",
    "        a (float): First number\n",
    "        b (float): Second number\n",
    "\n",
    "    Returns:\n",
    "        float: The result of a * b\n",
    "    \"\"\"\n",
    "    # Perform multiplication and return the result\n",
    "    return a * b\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Tool registry\n",
    "# ============================================================\n",
    "# Maps tool names (as strings) to their corresponding @tool objects.\n",
    "# This allows dynamic lookup and execution by name.\n",
    "\n",
    "tool_registry = {\n",
    "    \"add_numbers\": add_numbers,\n",
    "    \"subtract_numbers\": subtract_numbers,\n",
    "    \"multiply_numbers\": multiply_numbers\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Tool dispatch helper\n",
    "# ============================================================\n",
    "def _dispatch_tool(raw_name, args):\n",
    "    \"\"\"\n",
    "    Normalize the tool name, find the matching tool in the registry,\n",
    "    and call the underlying Python function.\n",
    "\n",
    "    Parameters:\n",
    "        raw_name (str): Tool name from the model (may vary in case)\n",
    "        args (dict): Arguments to pass to the tool\n",
    "\n",
    "    Returns:\n",
    "        Any: The result of the tool execution, or an error message if not found\n",
    "    \"\"\"\n",
    "    # Create a lowercase mapping of tool names for case-insensitive matching\n",
    "    name_map = {k.lower(): k for k in tool_registry.keys()}\n",
    "\n",
    "    # Check if the normalized name exists in the registry\n",
    "    if raw_name in name_map:\n",
    "        tool_key = name_map[raw_name]\n",
    "        # Call the original Python function stored in the @tool wrapper\n",
    "        return tool_registry[tool_key].func(**args)\n",
    "\n",
    "    # Return an error if the tool name is not recognized\n",
    "    return f\"Tool '{raw_name}' not recognized.\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Function call handler\n",
    "# ============================================================\n",
    "def execute_function_from_response(model_response):\n",
    "    \"\"\"\n",
    "    Check for a function call in the model's response and execute the matching tool.\n",
    "\n",
    "    Supports:\n",
    "        1. Native function_call objects (from APIs like OpenAI or Anthropic)\n",
    "        2. JSON-formatted text output (for backends like Ollama)\n",
    "\n",
    "    Parameters:\n",
    "        model_response: The model's output object\n",
    "\n",
    "    Returns:\n",
    "        Any: Tool execution result or plain text response\n",
    "    \"\"\"\n",
    "    # 1. Native function_call branch\n",
    "    function_call = model_response.additional_kwargs.get(\"function_call\")\n",
    "    if function_call:\n",
    "        name = function_call[\"name\"].strip().lower()\n",
    "        args = json.loads(function_call[\"arguments\"])\n",
    "        return _dispatch_tool(name, args)\n",
    "\n",
    "    # 2. Fallback: parse JSON from text output\n",
    "    try:\n",
    "        # Attempt to parse the model's content as JSON\n",
    "        data = json.loads(model_response.content)\n",
    "\n",
    "        # Extract tool name and arguments from the parsed JSON\n",
    "        name = data.get(\"name\", \"\").strip().lower()\n",
    "        args = data.get(\"arguments\", {})\n",
    "\n",
    "        # Dispatch to the correct tool\n",
    "        return _dispatch_tool(name, args)\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        # If parsing fails, no tool call was detected\n",
    "        print(\"No tools detected; returning plain text response.\")\n",
    "        return model_response.content\n",
    "\n",
    "# ============================================================\n",
    "# System prompt\n",
    "# ============================================================\n",
    "system_prompt = system_message(\n",
    "    \"You are a helpful assistant that can use tools to perform calculations.\\n\"\n",
    "    \"Available tools:\\n\"\n",
    "    \"1. add_numbers(a, b) — Return the sum of two numbers.\\n\"\n",
    "    \"2. subtract_numbers(a, b) — Return the difference between two numbers.\\n\"\n",
    "    \"3. multiply_numbers(a, b) — Return the product of two numbers.\\n\"\n",
    "    \"If a user's question requires math, respond with either:\\n\"\n",
    "    \" - A native function_call (if supported by your system), OR\\n\"\n",
    "    \" - A JSON object in this format:\\n\"\n",
    "    '{{\"name\": \"<exact_tool_name>\", \"arguments\": {{\"a\": <number>, \"b\": <number>}}}}\\n'\n",
    "    \"If no calculation is needed, respond normally in plain text.\"\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Example user questions\n",
    "# ============================================================\n",
    "user_questions = [\n",
    "    \"What is 15 minus 7?\",   # Should trigger subtract_numbers\n",
    "    \"Tell me a joke.\",       # Should return plain text\n",
    "    \"What is 13 times 32?\"   # Should trigger multiply_numbers\n",
    "]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Process each question\n",
    "# ============================================================\n",
    "for question in user_questions:\n",
    "    # Build the message sequence for this question\n",
    "    message_sequence = [system_prompt, user_message(question)]\n",
    "\n",
    "    # Create the conversation chain and append the function call handler\n",
    "    math_chain = create_conversation_chain(message_sequence) | RunnableLambda(execute_function_from_response)\n",
    "\n",
    "    # Invoke the chain and print the result\n",
    "    result = math_chain.invoke({})\n",
    "    print(f\"Q: {question}\\nA: {result}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ws-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
